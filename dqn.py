# -*- coding: utf-8 -*-
"""DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/spympr/car_racer_gym/blob/main/DQN.ipynb

## Install libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !sudo apt update && sudo apt install python-opengl
# !sudo apt update && sudo apt install xvfb
# !pip install gym-notebook-wrapper stable-baselines[mpi] box2d box2d-kengz pyvirtualdisplay pyglet
# !pip install gym
# !pip install wandb
# !pip install pyvirtualdisplay -qq
# !pip install folium==0.2.1
# !apt-get install -y xvfb python-opengl ffmpeg -qq

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !git clone https://github.com/openai/gym.git
# %cd gym
# !pip install -e .
# !pip install stable-baselines[mpi]
# !pip install stable-baselines3[extra]

"""## Import useful libraries"""

import math
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple
from itertools import count
from PIL import Image
import torch
import gnwrapper
import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(30)
import glob
import io
import os
import cv2
import base64
from collections import deque
from datetime import datetime
from IPython.display import HTML
from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
import wandb
import time

is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display

# if gpu is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""## Get a modified version of Car Racing from github official site"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/AGiannoutsos/car_racer_gym.git
# %cd car_racer_gym

from car_racing import CarRacingDiscrete

"""## Quick example with CarRacing"""

env = gnwrapper.Animation(CarRacingDiscrete())
env = CarRacingDiscrete()

env.reset()
env.render()
im = env.render("state_pixels")

def state_image_preprocess(state_image):
    state_image = state_image.transpose((2,0,1))
    state_image = np.ascontiguousarray(state_image, dtype=np.float32) / 255
    state_image = torch.from_numpy(state_image)
    return state_image.unsqueeze(0).to(device)

state_image_preprocess(im).shape
plt.imshow(state_image_preprocess(im).cpu().squeeze(0).permute(1, 2, 0).numpy())

"""## Implementation of DQN Custom Callback Class"""

from stable_baselines3.common.callbacks import BaseCallback

class DQNCustomCallback(BaseCallback):
    """
    A custom callback that derives from ``BaseCallback``.

    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug
    """
    def __init__(self, verbose=0):
        super(DQNCustomCallback, self).__init__(verbose)
        # Those variables will be accessible in the callback
        # (they are defined in the base class)
        # The RL model
        # self.model = None  # type: BaseAlgorithm
        # An alias for self.model.get_env(), the environment used for training
        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]
        # Number of time the callback was called
        # self.n_calls = 0  # type: int
        # self.num_timesteps = 0  # type: int
        # local and global variables
        # self.locals = None  # type: Dict[str, Any]
        # self.globals = None  # type: Dict[str, Any]
        # The logger object, used to report things in the terminal
        # self.logger = None  # stable_baselines3.common.logger
        # # Sometimes, for event callback, it is useful
        # # to have access to the parent object
        # self.parent = None  # type: Optional[BaseCallback]
        self.episodes = 0
        self.total_episode_reward = 0

    def _on_training_start(self) -> None:
        """
        This method is called before the first rollout starts.
        """
        pass

    def _on_rollout_start(self) -> None:
        """
        A rollout is the collection of environment interaction
        using the current policy.
        This event is triggered before collecting new samples.
        """
        pass

    def _on_step(self) -> bool:
        # update commulative reward to log at the end of every episode
        self.total_episode_reward += self.locals["reward"]
        # at the end of every episode
        if self.locals["done"][0]:

            # log the reward value if its time to not log 2 times
            if self.episodes % self.locals["log_interval"] != 0:
                wandb.log({"reward_per_episode": self.total_episode_reward})

            # if log interval has passed
            if self.episodes % self.locals["log_interval"] == 0:
                # log at wandb and print the last video
                # Save your model and optimizer
                self.model.save(MODEL_SAVE_NAME)
                # Save as artifact for version control.
                artifact = wandb.Artifact(MODEL_SAVE_NAME, type='model')
                artifact.add_file(MODEL_SAVE_NAME+".zip")
                wandb.log_artifact(artifact)
                wandb.log({"reward_per_episode": self.total_episode_reward})


                mp4list = glob.glob('video/*.mp4')
                print(mp4list)
                if len(mp4list) > 0:
                    print(len(mp4list))
                    mp4 = mp4list[-1]
                    video = io.open(mp4, 'r+b').read()
                    encoded = base64.b64encode(video)

                    # log gameplay video in wandb
                    wandb.log({"gameplays": wandb.Video(mp4, fps=4, format="gif")})

                    # display gameplay video
                    ipythondisplay.clear_output(wait=True)
                    ipythondisplay.display(HTML(data='''<video alt="" autoplay
                                loop controls style="height: 400px;">
                                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
                            </video>'''.format(encoded.decode('ascii'))))
                    print("Episode:", self.episodes)
            self.episodes += 1
            self.total_episode_reward = 0


        return True

    def _on_rollout_end(self) -> None:
        """
        This event is triggered before updating the policy.
        """
        pass

    def _on_training_end(self) -> None:
        """
        This event is triggered before exiting the `learn()` method.
        """
        pass

"""## Train our DQN model"""

from stable_baselines3 import DQN
from stable_baselines3.dqn import CnnPolicy

NUM = 54
NUM_OF_STEPS        = 5000
NUM_OF_EPISODES     = 1000
LOG_INTERVAL        = 50
BUFFER_SIZE         = 150000
LEARNING_STARTS     = 5000
WANDB_ID            = "spympr"+str(NUM)
WNDB_NAME           = "SPYROS"+str(NUM)
MODEL_SAVE_NAME     = "DQN_RL_"+str(NUM)
SAVED_MODEL_VERSION = "latest"
LOAD_SAVED_MODEL    = False

# Process to connect to weights and biases account
# wnadb api key: 00d5bfbd342bb73d5aaf4f2833436d20457ef040
os.environ["WANDB_ENTITY"]  = "andreas_giannoutsos"
os.environ["WANDB_PROJECT"] = "gym_car_racer"
os.environ["WANDB_RESUME"]  = "allow"
wandb.init(resume=WANDB_ID)
wandb.run.name = WNDB_NAME

# Use wrappers.Monitor in order to have a video
env = gym.wrappers.Monitor(CarRacingDiscrete(NUM_OF_STEPS), './video', force=True)

# Load model
if LOAD_SAVED_MODEL:
    try:
        model_artifact = wandb.use_artifact(MODEL_SAVE_NAME+':'+SAVED_MODEL_VERSION, type='model')
        artifact_dir = model_artifact.download()
        DQNmodel = DQN.load(artifact_dir+"/"+MODEL_SAVE_NAME, env=env)
        print("LOAD SAVED DQN MODEL")
    except:
        print("NO MODEL FOUND")
else:
    if 'DQNmodel' not in globals():
        DQNmodel = DQN(CnnPolicy, env, verbose=1, buffer_size=BUFFER_SIZE, learning_starts=LEARNING_STARTS)
        print("INITIALIZE NEW DQN MODEL")
    else:
        DQNmodel = DQN.load(MODEL_SAVE_NAME, env=env)
        print("CONTINUE DQN MODEL TRAINING")

# Train model
DQNmodel.learn(total_timesteps=NUM_OF_STEPS*NUM_OF_EPISODES, log_interval=LOG_INTERVAL, callback=DQNCustomCallback())

# Save model
DQNmodel.save(MODEL_SAVE_NAME)

"""## Evaluate our DQN model"""

def evaluate_version(model, env, version_name, version, video_path):
    # get version of model
    MODEL_SAVE_NAME     = version_name
    SAVED_MODEL_VERSION = version
    # wnadb api key: 00d5bfbd342bb73d5aaf4f2833436d20457ef040
    os.environ["WANDB_ENTITY"]  = "andreas_giannoutsos"
    os.environ["WANDB_PROJECT"] = "gym_car_racer"
    os.environ["WANDB_RESUME"]  = "allow"
    wandb.init()
    model_artifact = wandb.use_artifact(MODEL_SAVE_NAME+':'+SAVED_MODEL_VERSION, type='model')
    artifact_dir = model_artifact.download()
    loaded_model = model.load(artifact_dir+"/"+MODEL_SAVE_NAME)

    # play model
    env = gym.wrappers.Monitor(env, video_path, force=True)
    obs = env.reset()
    done = False
    while not done:
        action, _states = loaded_model.predict(obs,deterministic=True)
        obs, reward, done, info = env.step(action)
    env.close()

    # display video
    mp4list = glob.glob(video_path+'/*.mp4')
    print(mp4list)
    if len(mp4list) > 0:
        print(len(mp4list))
        mp4 = max(mp4list, key=os.path.getctime)
        # mp4 = mp4list[-1]
        video = io.open(mp4, 'r+b').read()
        encoded = base64.b64encode(video)

        # log gameplay video in wandb
        wandb.log({"gameplays": wandb.Video(mp4, fps=4, format="gif")})

        # display gameplay video
        ipythondisplay.clear_output(wait=True)
        ipythondisplay.display(HTML(data='''<video alt="" autoplay
                    loop controls style="height: 400px;">
                    <source src="data:video/mp4;base64,{0}" type="video/mp4" />
                </video>'''.format(encoded.decode('ascii'))))
        print("Video path: ", mp4)

from stable_baselines3 import DQN

env = CarRacingDiscrete(1000)

# Model with 10.000 steps has best performance at 566 episode (566/20~=28)
evaluate_version(DQN, env, "DQN_RL_24", "v28", "./videoo")

# Model with 2.000 steps has best performance at 798 episode (800/50~=15)
# evaluate_version(DQN, env, "DQN_RL_48", "v15", "./videoo")

# Model with 5.000 steps has best performance at last episode
# evaluate_version(DQN, env, "DQN_RL_54", "v9", "./videoo")